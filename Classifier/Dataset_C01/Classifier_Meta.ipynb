{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ensure the sample counts of two datasets are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image IDs in fused features but not in emotional features: set()\n",
      "Image IDs in emotional_features but not in fused features: set()\n",
      "Aligned Fused features:\n",
      "                                         0         1         2         3  \\\n",
      "image_id                                                                   \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o -0.521330  0.065525 -0.028026  0.187444   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.482425  0.079567 -0.036039  0.194375   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0 -0.468386  0.084869 -0.049615  0.144983   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.468805  0.067019 -0.031303  0.199792   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.473862  0.089600 -0.023923  0.144352   \n",
      "\n",
      "                                         4         5         6         7  \\\n",
      "image_id                                                                   \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o  0.023185  0.102659 -0.126628 -0.045669   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j  0.036827  0.083796 -0.112961  0.000671   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0  0.038398  0.096167 -0.096763 -0.018022   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k  0.033397  0.084818 -0.119774 -0.028753   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw  0.033620  0.057612 -0.104852 -0.013622   \n",
      "\n",
      "                                         8         9  ...      1526      1527  \\\n",
      "image_id                                              ...                       \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o  0.036374  0.142169  ... -0.004261 -0.239570   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.050403  0.172671  ...  0.002555 -0.178980   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0  0.016321  0.148485  ...  0.019371 -0.167772   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.030460  0.144257  ... -0.029512 -0.165330   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.017427  0.127003  ... -0.002859 -0.209840   \n",
      "\n",
      "                                      1528      1529      1530      1531  \\\n",
      "image_id                                                                   \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o -0.247390 -0.061778  0.205731  0.027570   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.252651 -0.049785  0.185724  0.014776   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0 -0.270531 -0.109937  0.200574 -0.002043   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.279122 -0.071788  0.210830 -0.011106   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.229515 -0.065925  0.211815  0.016175   \n",
      "\n",
      "                                      1532      1533      1534      1535  \n",
      "image_id                                                                  \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o -0.133678 -0.054258 -0.177480  0.268748  \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.098063 -0.031113 -0.181574  0.291177  \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0 -0.114087 -0.050619 -0.196107  0.234879  \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.118139 -0.057057 -0.175628  0.264600  \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.121006 -0.004409 -0.176248  0.259960  \n",
      "\n",
      "[5 rows x 1536 columns]\n",
      "\n",
      "Aligned Emotional features:\n",
      "                                    neg    neu    pos  compound\n",
      "image_id                                                       \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o  0.105  0.895  0.000   -0.8442\n",
      "563a2b53jw1exl77nkup7j20c30f3q4j  0.000  0.868  0.132    0.5719\n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0  0.000  0.911  0.089    0.5994\n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k  0.147  0.827  0.026   -0.8360\n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw  0.065  0.793  0.141    0.7550\n",
      "Fused features sample count: 5413\n",
      "Emotional features sample count: 5413\n",
      "The sample counts for fused features and emotional features are consistent.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载并对齐融合特征和情感特征\n",
    "def load_and_align_features(fused_path, emotional_path):\n",
    "    # 加载融合特征\n",
    "    fused_features_df = pd.read_csv(fused_path)\n",
    "    fused_features_df.set_index('image_id', inplace=True)\n",
    "    \n",
    "    # 加载情感特征\n",
    "    emotional_features_df = pd.read_csv(emotional_path, index_col=0)\n",
    "    \n",
    "    # 确保索引类型一致\n",
    "    fused_features_df.index = fused_features_df.index.astype(str)\n",
    "    emotional_features_df.index = emotional_features_df.index.astype(str)\n",
    "    \n",
    "    # 删除重复值\n",
    "    fused_features_df = fused_features_df[~fused_features_df.index.duplicated(keep='first')]\n",
    "    emotional_features_df = emotional_features_df[~emotional_features_df.index.duplicated(keep='first')]\n",
    "    \n",
    "    # 找到共同的 image_id\n",
    "    common_image_ids = fused_features_df.index.intersection(emotional_features_df.index)\n",
    "    \n",
    "    # 打印调试信息\n",
    "    print(f\"Image IDs in fused features but not in emotional features: {set(fused_features_df.index) - set(emotional_features_df.index)}\")\n",
    "    print(f\"Image IDs in emotional_features but not in fused features: {set(emotional_features_df.index) - set(fused_features_df.index)}\")\n",
    "    \n",
    "    # 过滤数据以保留共同的 image_id\n",
    "    fused_features_df = fused_features_df.loc[common_image_ids]\n",
    "    emotional_features_df = emotional_features_df.loc[common_image_ids]\n",
    "    \n",
    "    return fused_features_df, emotional_features_df\n",
    "\n",
    "# 文件路径\n",
    "fused_features_path = 'fused.csv'\n",
    "emotional_features_path = 'SA_01.csv'\n",
    "\n",
    "# 执行第一步\n",
    "fused_features_df, emotional_features_df = load_and_align_features(fused_features_path, emotional_features_path)\n",
    "\n",
    "# 打印前几行检查\n",
    "print(\"Aligned Fused features:\")\n",
    "print(fused_features_df.head())\n",
    "\n",
    "print(\"\\nAligned Emotional features:\")\n",
    "print(emotional_features_df.head())\n",
    "\n",
    "\n",
    "# Check if the sample counts are consistent\n",
    "fused_sample_count = fused_features_df.shape[0]\n",
    "emotional_sample_count = emotional_features_df.shape[0]\n",
    "\n",
    "print(f'Fused features sample count: {fused_sample_count}')\n",
    "print(f'Emotional features sample count: {emotional_sample_count}')\n",
    "\n",
    "if fused_sample_count != emotional_sample_count:\n",
    "    print(\"Warning: The sample counts for fused features and emotional features are not consistent.\")\n",
    "else:\n",
    "    print(\"The sample counts for fused features and emotional features are consistent.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data alignment - merge through 'image_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fused features with labels:\n",
      "                                         0         1         2         3  \\\n",
      "image_id                                                                   \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o -0.521330  0.065525 -0.028026  0.187444   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.482425  0.079567 -0.036039  0.194375   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0 -0.468386  0.084869 -0.049615  0.144983   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.468805  0.067019 -0.031303  0.199792   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.473862  0.089600 -0.023923  0.144352   \n",
      "\n",
      "                                         4         5         6         7  \\\n",
      "image_id                                                                   \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o  0.023185  0.102659 -0.126628 -0.045669   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j  0.036827  0.083796 -0.112961  0.000671   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0  0.038398  0.096167 -0.096763 -0.018022   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k  0.033397  0.084818 -0.119774 -0.028753   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw  0.033620  0.057612 -0.104852 -0.013622   \n",
      "\n",
      "                                         8         9  ...      1527      1528  \\\n",
      "image_id                                              ...                       \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o  0.036374  0.142169  ... -0.239570 -0.247390   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.050403  0.172671  ... -0.178980 -0.252651   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0  0.016321  0.148485  ... -0.167772 -0.270531   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.030460  0.144257  ... -0.165330 -0.279122   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.017427  0.127003  ... -0.209840 -0.229515   \n",
      "\n",
      "                                      1529      1530      1531      1532  \\\n",
      "image_id                                                                   \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o -0.061778  0.205731  0.027570 -0.133678   \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.049785  0.185724  0.014776 -0.098063   \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0 -0.109937  0.200574 -0.002043 -0.114087   \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.071788  0.210830 -0.011106 -0.118139   \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.065925  0.211815  0.016175 -0.121006   \n",
      "\n",
      "                                      1533      1534      1535  label  \n",
      "image_id                                                               \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o -0.054258 -0.177480  0.268748      0  \n",
      "563a2b53jw1exl77nkup7j20c30f3q4j -0.031113 -0.181574  0.291177      0  \n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0 -0.050619 -0.196107  0.234879      0  \n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k -0.057057 -0.175628  0.264600      0  \n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.004409 -0.176248  0.259960      0  \n",
      "\n",
      "[5 rows x 1537 columns]\n",
      "\n",
      "Emotional features with labels:\n",
      "                                    neg    neu    pos  compound  label\n",
      "image_id                                                              \n",
      "62b31d36gw1expsi2gfrdj20hm0loq8o  0.105  0.895  0.000   -0.8442      0\n",
      "563a2b53jw1exl77nkup7j20c30f3q4j  0.000  0.868  0.132    0.5719      0\n",
      "005ldo0ygw1ex23rdfuqcj30xo0k6di0  0.000  0.911  0.089    0.5994      0\n",
      "62b31d36gw1exfcmyz8agj20qq0hu77k  0.147  0.827  0.026   -0.8360      0\n",
      "0060kjm0jw1exdjaeiqadj30xc0m8tdw  0.065  0.793  0.141    0.7550      0\n",
      "Fused features with labels sample count: 5413\n",
      "Emotional features with labels sample count: 5413\n",
      "The sample counts for fused features and emotional features with labels are consistent.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load labels from pickle file\n",
    "def load_labels_from_pickle(pickle_file):\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        image_ids = data['image_id']\n",
    "        labels = data['label']\n",
    "        # Ensure image_ids are of string type\n",
    "        image_ids = [str(image_id) for image_id in image_ids]\n",
    "        labels_dict = dict(zip(image_ids, labels))\n",
    "    return labels_dict\n",
    "\n",
    "# Merge features with labels\n",
    "def merge_with_labels(features_df, labels_dict):\n",
    "    labels_df = pd.DataFrame(list(labels_dict.items()), columns=['image_id', 'label'])\n",
    "    labels_df.set_index('image_id', inplace=True)\n",
    "    \n",
    "    # Ensure indices are of the same type\n",
    "    features_df.index = features_df.index.astype(str)\n",
    "    labels_df.index = labels_df.index.astype(str)\n",
    "    \n",
    "    # Merge features with labels\n",
    "    merged_df = pd.merge(features_df, labels_df, left_index=True, right_index=True, how='inner')\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# File path for labels\n",
    "pickle_path = 'train.pkl'\n",
    "\n",
    "# Load labels\n",
    "labels_dict = load_labels_from_pickle(pickle_path)\n",
    "\n",
    "# Ensure the image_id columns in fused_features_df and emotional_features_df are strings\n",
    "fused_features_df.index = fused_features_df.index.astype(str)\n",
    "emotional_features_df.index = emotional_features_df.index.astype(str)\n",
    "\n",
    "# Merge with labels\n",
    "fused_merged_df = merge_with_labels(fused_features_df, labels_dict)\n",
    "emotional_merged_df = merge_with_labels(emotional_features_df, labels_dict)\n",
    "\n",
    "# Print the first few rows to check\n",
    "print(\"Fused features with labels:\")\n",
    "print(fused_merged_df.head())\n",
    "\n",
    "print(\"\\nEmotional features with labels:\")\n",
    "print(emotional_merged_df.head())\n",
    "\n",
    "# Check if the sample counts are consistent after merging with labels\n",
    "fused_sample_count = fused_merged_df.shape[0]\n",
    "emotional_sample_count = emotional_merged_df.shape[0]\n",
    "\n",
    "print(f'Fused features with labels sample count: {fused_sample_count}')\n",
    "print(f'Emotional features with labels sample count: {emotional_sample_count}')\n",
    "\n",
    "if fused_sample_count != emotional_sample_count:\n",
    "    print(\"Warning: The sample counts for fused features and emotional features with labels are not consistent.\")\n",
    "else:\n",
    "    print(\"The sample counts for fused features and emotional features with labels are consistent.\")\n",
    "\n",
    "# Save the merged dataframes to CSV files\n",
    "# fused_merged_df.to_csv('fused_with_labels.csv')\n",
    "# emotional_merged_df.to_csv('emotional_with_labels.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split train & test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Split the data into features and labels\n",
    "X_fused = fused_merged_df.reset_index().drop('label', axis=1)\n",
    "y_fused = fused_merged_df['label']\n",
    "\n",
    "X_emotional = emotional_merged_df.reset_index().drop('label', axis=1)\n",
    "y_emotional = emotional_merged_df['label']\n",
    "\n",
    "# Split the datasets into training and testing sets\n",
    "X_fused_train, X_fused_test, y_fused_train, y_fused_test = train_test_split(X_fused, y_fused, test_size=0.2, random_state=42)\n",
    "X_emotional_train, X_emotional_test, y_emotional_train, y_emotional_test = train_test_split(X_emotional, y_emotional, test_size=0.2, random_state=42)\n",
    "\n",
    "# Store image_id for merging later\n",
    "image_id_train_f = X_fused_train['image_id']\n",
    "image_id_test_f= X_fused_test['image_id']\n",
    "\n",
    "image_id_train_e = X_emotional_train['image_id']\n",
    "image_id_test_e = X_emotional_test['image_id']\n",
    "\n",
    "\n",
    "# Remove image_id from the training and testing sets\n",
    "X_fused_train = X_fused_train.drop('image_id', axis=1)\n",
    "X_fused_test = X_fused_test.drop('image_id', axis=1)\n",
    "X_emotional_train = X_emotional_train.drop('image_id', axis=1)\n",
    "X_emotional_test = X_emotional_test.drop('image_id', axis=1)\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler_fused = StandardScaler()\n",
    "X_fused_train = scaler_fused.fit_transform(X_fused_train)\n",
    "X_fused_test = scaler_fused.transform(X_fused_test)\n",
    "\n",
    "scaler_emotional = StandardScaler()\n",
    "X_emotional_train = scaler_emotional.fit_transform(X_emotional_train)\n",
    "X_emotional_test = scaler_emotional.transform(X_emotional_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training dimensions: 200\n",
      "New testing dimensions: 200\n",
      "Total explained variance: 0.9493278247524244\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 应用 PCA 降维，只在训练数据上fit，然后transform训练数据和测试数据\n",
    "pca = PCA(n_components=200)  # 直接指定主成分数量\n",
    "X_fused_train_pca = pca.fit_transform(X_fused_train) # pca后的训练集\n",
    "X_fused_test_pca = pca.transform(X_fused_test) # pca后的测试集\n",
    "\n",
    "# 检查新的维度和解释的方差比\n",
    "print(\"New training dimensions:\", X_fused_train_pca.shape[1])\n",
    "print(\"New testing dimensions:\", X_fused_test_pca.shape[1])\n",
    "# print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Total explained variance:\", np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN values in y_fused_train: 0\n",
      "NaN values in y_fused_test: 0\n"
     ]
    }
   ],
   "source": [
    "# 检查训练和测试标签中的 NaN 值\n",
    "print(\"NaN values in y_fused_train:\", y_fused_train.isna().sum())\n",
    "print(\"NaN values in y_fused_test:\", y_fused_test.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 删除包含 NaN 值的样本\n",
    "X_fused_train = X_fused_train[y_fused_train.notna()]\n",
    "y_fused_train = y_fused_train.dropna()\n",
    "\n",
    "X_fused_test = X_fused_test[y_fused_test.notna()]\n",
    "y_fused_test = y_fused_test.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLP - 4 hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.9840646651270207\n",
      "Testing accuracy: 0.6103416435826408\n",
      "\n",
      "Classification Report for Training Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      1999\n",
      "           1       0.99      0.98      0.99      2331\n",
      "\n",
      "    accuracy                           0.98      4330\n",
      "   macro avg       0.98      0.98      0.98      4330\n",
      "weighted avg       0.98      0.98      0.98      4330\n",
      "\n",
      "\n",
      "Classification Report for Testing Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.60      0.60       516\n",
      "           1       0.63      0.62      0.62       567\n",
      "\n",
      "    accuracy                           0.61      1083\n",
      "   macro avg       0.61      0.61      0.61      1083\n",
      "weighted avg       0.61      0.61      0.61      1083\n",
      "\n",
      "fused_probs_train: [[1.39781401e-01 8.60218599e-01]\n",
      " [9.99999624e-01 3.75778251e-07]\n",
      " [9.99996572e-01 3.42779744e-06]\n",
      " [9.99994032e-01 5.96802291e-06]\n",
      " [3.57320608e-05 9.99964268e-01]]\n",
      "fused_probs_test: [[9.99851587e-01 1.48413271e-04]\n",
      " [5.89826050e-01 4.10173950e-01]\n",
      " [9.99802856e-01 1.97144445e-04]\n",
      " [3.97606237e-03 9.96023938e-01]\n",
      " [9.98286013e-01 1.71398718e-03]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# 根据最佳参数设置MLP分类器\n",
    "mlp_fused = MLPClassifier(\n",
    "    hidden_layer_sizes=(200,100,50,25,),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.01,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    random_state=42,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# 创建Pipeline，包含标准化和PCA\n",
    "pipeline_fused = make_pipeline(StandardScaler(), pca, mlp_fused)\n",
    "\n",
    "# 训练模型\n",
    "pipeline_fused.fit(X_fused_train, y_fused_train)\n",
    "\n",
    "# 预测训练集和测试集\n",
    "mlp_train_preds = pipeline_fused.predict(X_fused_train)\n",
    "mlp_test_preds = pipeline_fused.predict(X_fused_test)\n",
    "\n",
    "# 计算并打印训练集和测试集的准确率\n",
    "mlp_train_accuracy = accuracy_score(y_fused_train, mlp_train_preds)\n",
    "mlp_test_accuracy = accuracy_score(y_fused_test, mlp_test_preds)\n",
    "\n",
    "print(f'Training accuracy: {mlp_train_accuracy}')\n",
    "print(f'Testing accuracy: {mlp_test_accuracy}')\n",
    "\n",
    "# 打印分类报告\n",
    "print(\"\\nClassification Report for Training Set:\")\n",
    "print(classification_report(y_fused_train, mlp_train_preds))\n",
    "\n",
    "print(\"\\nClassification Report for Testing Set:\")\n",
    "print(classification_report(y_fused_test, mlp_test_preds))\n",
    "\n",
    "# 计算训练集和测试集的预测概率\n",
    "fused_probs_train = pipeline_fused.predict_proba(X_fused_train)\n",
    "fused_probs_test = pipeline_fused.predict_proba(X_fused_test)\n",
    "\n",
    "# check \n",
    "print(f'fused_probs_train: {fused_probs_train[:5]}')\n",
    "print(f'fused_probs_test: {fused_probs_test[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emtional Tensor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.7889145496535797\n",
      "Testing accuracy: 0.5752539242843951\n",
      "\n",
      "Classification Report for Training Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.72      0.76      1999\n",
      "           1       0.78      0.85      0.81      2331\n",
      "\n",
      "    accuracy                           0.79      4330\n",
      "   macro avg       0.79      0.78      0.79      4330\n",
      "weighted avg       0.79      0.79      0.79      4330\n",
      "\n",
      "\n",
      "Classification Report for Testing Set:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.46      0.51       516\n",
      "           1       0.58      0.68      0.63       567\n",
      "\n",
      "    accuracy                           0.58      1083\n",
      "   macro avg       0.57      0.57      0.57      1083\n",
      "weighted avg       0.57      0.58      0.57      1083\n",
      "\n",
      "emotional_probs_train: [[0.44559024 0.55440976]\n",
      " [0.59227008 0.40772992]\n",
      " [0.44709619 0.55290381]\n",
      " [0.49111273 0.50888727]\n",
      " [0.42279922 0.57720078]]\n",
      "emotional_probs_test: [[0.35442145 0.64557855]\n",
      " [0.53556222 0.46443778]\n",
      " [0.47788084 0.52211916]\n",
      " [0.56942083 0.43057917]\n",
      " [0.57157755 0.42842245]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# 创建随机森林分类器，使用已知的最佳参数\n",
    "rf_emotional = RandomForestClassifier(\n",
    "               max_depth=10, \n",
    "               min_samples_split=2, \n",
    "               n_estimators=100, \n",
    "               random_state=42)\n",
    "\n",
    "# 创建Pipeline，包含标准化\n",
    "pipeline_emotional = make_pipeline(StandardScaler(),rf_emotional)\n",
    "\n",
    "# 使用训练数据拟合模型\n",
    "pipeline_emotional.fit(X_emotional_train, y_emotional_train)\n",
    "\n",
    "# 预测训练集和测试集\n",
    "rf_train_preds = pipeline_emotional.predict(X_emotional_train)\n",
    "rf_test_preds = pipeline_emotional.predict(X_emotional_test)\n",
    "\n",
    "# 计算并打印训练集和测试集的准确率\n",
    "rf_train_accuracy = accuracy_score(y_emotional_train, rf_train_preds)\n",
    "rf_test_accuracy = accuracy_score(y_emotional_test, rf_test_preds)\n",
    "\n",
    "print(f'Training accuracy: {rf_train_accuracy}')\n",
    "print(f'Testing accuracy: {rf_test_accuracy}')\n",
    "\n",
    "# 打印分类报告\n",
    "print(\"\\nClassification Report for Training Set:\")\n",
    "print(classification_report(y_emotional_train, rf_train_preds))\n",
    "\n",
    "print(\"\\nClassification Report for Testing Set:\")\n",
    "print(classification_report(y_emotional_test, rf_test_preds))\n",
    "\n",
    "emotional_probs_train = pipeline_emotional.predict_proba(X_emotional_train)\n",
    "emotional_probs_test = pipeline_emotional.predict_proba(X_emotional_test)\n",
    "\n",
    "print(f'emotional_probs_train: {emotional_probs_train[:5]}')\n",
    "print(f'emotional_probs_test: {emotional_probs_test[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fused_prob_0  fused_prob_1                          image_id  \\\n",
      "0      0.139781  8.602186e-01  6693ce84gw1eyfsiy3rptj20c80960u6   \n",
      "1      1.000000  3.757783e-07  652f5916jw1ezod9f7u5tj20c80963yl   \n",
      "2      0.999997  3.427797e-06  75b52ed2jw1ewcdnpunn9j20r80kfadv   \n",
      "3      0.999994  5.968023e-06  63207a53jw1eyffma5xquj20c807gglz   \n",
      "4      0.000036  9.999643e-01  6bdb0c34jw1eqqhdw68loj20hs0vkgpy   \n",
      "\n",
      "   emotional_prob_0  emotional_prob_1  label  \n",
      "0          0.445590          0.554410      0  \n",
      "1          0.592270          0.407730      0  \n",
      "2          0.447096          0.552904      0  \n",
      "3          0.491113          0.508887      0  \n",
      "4          0.422799          0.577201      1  \n",
      "   fused_prob_0  fused_prob_1                          image_id  \\\n",
      "0      0.999852      0.000148  620beb06gw1eymgpmehb8j20b308bweh   \n",
      "1      0.589826      0.410174  6e05a6dagw1evwh8gzxnpj20hs0tx798   \n",
      "2      0.999803      0.000197  0061d424jw1eqjwrzpv8dj30go0goq6j   \n",
      "3      0.003976      0.996024  644471aegw1ez9osj60jxj20c80c840m   \n",
      "4      0.998286      0.001714            6ceb7622jw1e1h3mo4pvvj   \n",
      "\n",
      "   emotional_prob_0  emotional_prob_1  label  \n",
      "0          0.354421          0.645579      0  \n",
      "1          0.535562          0.464438      0  \n",
      "2          0.477881          0.522119      1  \n",
      "3          0.569421          0.430579      0  \n",
      "4          0.571578          0.428422      1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 确保 image_id 列都是字符串类型\n",
    "image_id_train_f = [str(image_id) for image_id in image_id_train_f]\n",
    "image_id_train_e = [str(image_id) for image_id in image_id_train_e]\n",
    "image_id_test_f = [str(image_id) for image_id in image_id_test_f]\n",
    "image_id_test_e = [str(image_id) for image_id in image_id_test_e]\n",
    "\n",
    "# 预测所有类别的概率\n",
    "fused_probs_train = pipeline_fused.predict_proba(X_fused_train)\n",
    "fused_probs_test = pipeline_fused.predict_proba(X_fused_test)\n",
    "emotional_probs_train = rf_emotional.predict_proba(X_emotional_train)\n",
    "emotional_probs_test = rf_emotional.predict_proba(X_emotional_test)\n",
    "\n",
    "# 获取类别标签\n",
    "class_labels_fused = pipeline_fused.classes_\n",
    "class_labels_emotional = rf_emotional.classes_\n",
    "\n",
    "# 将概率转换为数据框，并添加列名\n",
    "df_fused_probs_train = pd.DataFrame(fused_probs_train, columns=[f'fused_prob_{cls}' for cls in class_labels_fused])\n",
    "df_fused_probs_test = pd.DataFrame(fused_probs_test, columns=[f'fused_prob_{cls}' for cls in class_labels_fused])\n",
    "df_emotional_probs_train = pd.DataFrame(emotional_probs_train, columns=[f'emotional_prob_{cls}' for cls in class_labels_emotional])\n",
    "df_emotional_probs_test = pd.DataFrame(emotional_probs_test, columns=[f'emotional_prob_{cls}' for cls in class_labels_emotional])\n",
    "\n",
    "# 打印前几行检查\n",
    "# print(f'df_fused_probs_train:')\n",
    "# print(df_fused_probs_train.head())\n",
    "\n",
    "# print(f'df_fused_probs_test:')\n",
    "# print(df_fused_probs_test.head())\n",
    "\n",
    "# print(f'df_emotional_probs_train:')\n",
    "# print(df_emotional_probs_train.head())\n",
    "\n",
    "# print(f'df_emotional_probs_test:')\n",
    "# print(df_emotional_probs_test.head())\n",
    "\n",
    "\n",
    "# 添加 image_id 列\n",
    "df_fused_probs_train['image_id'] = image_id_train_f\n",
    "df_fused_probs_test['image_id'] = image_id_test_f\n",
    "df_emotional_probs_train['image_id'] = image_id_train_e\n",
    "df_emotional_probs_test['image_id'] = image_id_test_e\n",
    "\n",
    "# 创建包含 image_id 和标签的数据框\n",
    "df_label_train = pd.DataFrame({\n",
    "    'image_id': image_id_train_f,  # 假设训练集中的 image_id 与 fused_train 的相同\n",
    "    'label': y_fused_train\n",
    "})\n",
    "\n",
    "df_label_test = pd.DataFrame({\n",
    "    'image_id': image_id_test_f,  # 假设测试集中的 image_id 与 fused_test 的相同\n",
    "    'label': y_fused_test\n",
    "})\n",
    "\n",
    "# 在合并之前重置索引\n",
    "df_fused_probs_train = df_fused_probs_train.reset_index(drop=True)\n",
    "df_emotional_probs_train = df_emotional_probs_train.reset_index(drop=True)\n",
    "df_fused_probs_test = df_fused_probs_test.reset_index(drop=True)\n",
    "df_emotional_probs_test = df_emotional_probs_test.reset_index(drop=True)\n",
    "df_label_train = df_label_train.reset_index(drop=True)\n",
    "df_label_test = df_label_test.reset_index(drop=True)\n",
    "\n",
    "# 通过 image_id 对齐训练集数据\n",
    "meta_train_df = pd.merge(df_fused_probs_train, df_emotional_probs_train, on='image_id')\n",
    "meta_train_df = pd.merge(meta_train_df, df_label_train, on='image_id')\n",
    "\n",
    "# 通过 image_id 对齐测试集数据\n",
    "meta_test_df = pd.merge(df_fused_probs_test, df_emotional_probs_test, on='image_id')\n",
    "meta_test_df = pd.merge(meta_test_df, df_label_test, on='image_id')\n",
    "\n",
    "# 检查合并后的数据框\n",
    "print(meta_train_df.head())\n",
    "print(meta_test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61865189289012\n",
      "Precision: 0.6189834518384246\n",
      "Recall: 0.61865189289012\n",
      "F1 Score: 0.6187749793108587\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.61      0.60       516\n",
      "           1       0.64      0.63      0.63       567\n",
      "\n",
      "    accuracy                           0.62      1083\n",
      "   macro avg       0.62      0.62      0.62      1083\n",
      "weighted avg       0.62      0.62      0.62      1083\n",
      "\n",
      "0\n",
      "   fused_prob_0  fused_prob_1                          image_id  \\\n",
      "0      0.999852      0.000148  620beb06gw1eymgpmehb8j20b308bweh   \n",
      "1      0.589826      0.410174  6e05a6dagw1evwh8gzxnpj20hs0tx798   \n",
      "2      0.999803      0.000197  0061d424jw1eqjwrzpv8dj30go0goq6j   \n",
      "3      0.003976      0.996024  644471aegw1ez9osj60jxj20c80c840m   \n",
      "4      0.998286      0.001714            6ceb7622jw1e1h3mo4pvvj   \n",
      "\n",
      "   emotional_prob_0  emotional_prob_1  label  mlp_pred  \n",
      "0          0.354421          0.645579      0         0  \n",
      "1          0.535562          0.464438      0         0  \n",
      "2          0.477881          0.522119      1         0  \n",
      "3          0.569421          0.430579      0         1  \n",
      "4          0.571578          0.428422      1         0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "\n",
    "# 假设 meta_train_df 和 meta_test_df 已经定义\n",
    "\n",
    "# 使用 MLP 进行训练\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42, solver='lbfgs')\n",
    "\n",
    "# 训练 MLP 模型\n",
    "X_train = meta_train_df[['fused_prob_1', 'emotional_prob_1']]\n",
    "y_train = meta_train_df['label']\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# 预测所有样本的标签\n",
    "X_test = meta_test_df[['fused_prob_1', 'emotional_prob_1']]\n",
    "y_test = meta_test_df['label']\n",
    "mlp_predictions = mlp.predict(X_test)\n",
    "\n",
    "# 检查预测结果是否包含 NaN\n",
    "if any(pd.isna(mlp_predictions)):\n",
    "    print(\"MLP 预测结果中包含 NaN 值\")\n",
    "\n",
    "# 添加 'mlp_pred' 列    \n",
    "meta_test_df['mlp_pred'] = mlp_predictions\n",
    "\n",
    "# 计算并输出评估指标\n",
    "accuracy = accuracy_score(y_test, mlp_predictions)\n",
    "precision = precision_score(y_test, mlp_predictions, average='weighted')\n",
    "recall = recall_score(y_test, mlp_predictions, average='weighted')\n",
    "f1 = f1_score(y_test, mlp_predictions, average='weighted')\n",
    "classification_rep = classification_report(y_test, mlp_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# 检查预测结果是否包含 NaN 值并打印前几行数据\n",
    "print(meta_test_df['mlp_pred'].isnull().sum())\n",
    "print(meta_test_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.615881809787627\n",
      "Precision: 0.6160967442173018\n",
      "Recall: 0.615881809787627\n",
      "F1 Score: 0.6159703870360157\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.60      0.60       516\n",
      "           1       0.63      0.63      0.63       567\n",
      "\n",
      "    accuracy                           0.62      1083\n",
      "   macro avg       0.62      0.62      0.62      1083\n",
      "weighted avg       0.62      0.62      0.62      1083\n",
      "\n",
      "0\n",
      "   fused_prob_0  fused_prob_1                          image_id  \\\n",
      "0      0.999852      0.000148  620beb06gw1eymgpmehb8j20b308bweh   \n",
      "1      0.589826      0.410174  6e05a6dagw1evwh8gzxnpj20hs0tx798   \n",
      "2      0.999803      0.000197  0061d424jw1eqjwrzpv8dj30go0goq6j   \n",
      "3      0.003976      0.996024  644471aegw1ez9osj60jxj20c80c840m   \n",
      "4      0.998286      0.001714            6ceb7622jw1e1h3mo4pvvj   \n",
      "\n",
      "   emotional_prob_0  emotional_prob_1  label  mlp_pred  log_reg_pred  \n",
      "0          0.354421          0.645579      0         0             0  \n",
      "1          0.535562          0.464438      0         0             0  \n",
      "2          0.477881          0.522119      1         0             0  \n",
      "3          0.569421          0.430579      0         1             1  \n",
      "4          0.571578          0.428422      1         0             0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# 使用逻辑回归进行训练\n",
    "log_reg = LogisticRegression(max_iter=100, random_state=42)\n",
    "\n",
    "# 训练逻辑回归模型\n",
    "X_train = meta_train_df[['fused_prob_1', 'emotional_prob_1']]\n",
    "y_train = meta_train_df['label']\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "# 预测所有样本的标签\n",
    "X_test = meta_test_df[['fused_prob_1', 'emotional_prob_1']]\n",
    "y_test = meta_test_df['label']\n",
    "log_reg_predictions = log_reg.predict(X_test)\n",
    "\n",
    "# 检查预测结果是否包含 NaN\n",
    "if any(pd.isna(log_reg_predictions)):\n",
    "    print(\"逻辑回归预测结果中包含 NaN 值\")\n",
    "\n",
    "# 添加 'log_reg_pred' 列    \n",
    "meta_test_df['log_reg_pred'] = log_reg_predictions\n",
    "\n",
    "# 计算并输出评估指标\n",
    "accuracy = accuracy_score(y_test, log_reg_predictions)\n",
    "precision = precision_score(y_test, log_reg_predictions, average='weighted')\n",
    "recall = recall_score(y_test, log_reg_predictions, average='weighted')\n",
    "f1 = f1_score(y_test, log_reg_predictions, average='weighted')\n",
    "classification_rep = classification_report(y_test, log_reg_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# 检查预测结果是否包含 NaN 值并打印前几行数据\n",
    "print(meta_test_df['log_reg_pred'].isnull().sum())\n",
    "print(meta_test_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
