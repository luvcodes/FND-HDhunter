{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the fused fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# 加载特征数据\n",
    "# features_path = 'D:\\\\CAPSTONE5703_CNN\\\\final02.csv'\n",
    "# features_df = pd.read_csv(features_path)\n",
    "\n",
    "# 打印总行数和总列数\n",
    "# print(f\"总行数: {features_df.shape[0]}\")\n",
    "# print(f\"总列数: {features_df.shape[1]}\")\n",
    "\n",
    "# 打印前五行数据\n",
    "# print(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import pickle\n",
    "\n",
    "# def load_labels_from_pickle(pickle_file):\n",
    "    # with open(pickle_file, 'rb') as f:\n",
    "        # data = pickle.load(f)\n",
    "        # labels = data['label']\n",
    "    # return labels\n",
    "\n",
    "# 初始化 PickleDataset\n",
    "# pickle_path = 'D:\\\\CAPSTONE5703_CNN\\\\datasets_pickle\\\\train.pkl'\n",
    "# labels = load_labels_from_pickle(pickle_path)\n",
    "\n",
    "# 将标签转换为 DataFrame\n",
    "# labels_df = pd.DataFrame({'label': labels})\n",
    "\n",
    "# 打印标签总行数\n",
    "# print(f\"Total number of rows of labels: {len(labels_df)}\")\n",
    "\n",
    "# 打印前 5 行标签\n",
    "# print(\"First 5 rows of labels:\")\n",
    "# print(labels_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data alignment - merge through 'image_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           image_id         0         1         2         3  \\\n",
      "0  62b31d36gw1expsi2gfrdj20hm0loq8o -0.521330  0.065525 -0.028026  0.187444   \n",
      "1  563a2b53jw1exl77nkup7j20c30f3q4j -0.482425  0.079567 -0.036039  0.194375   \n",
      "2  005ldo0ygw1ex23rdfuqcj30xo0k6di0 -0.468386  0.084869 -0.049615  0.144983   \n",
      "3  62b31d36gw1exfcmyz8agj20qq0hu77k -0.468805  0.067019 -0.031303  0.199792   \n",
      "4  0060kjm0jw1exdjaeiqadj30xc0m8tdw -0.473862  0.089600 -0.023923  0.144352   \n",
      "\n",
      "          4         5         6         7         8  ...      1527      1528  \\\n",
      "0  0.023185  0.102659 -0.126628 -0.045669  0.036374  ... -0.239570 -0.247390   \n",
      "1  0.036827  0.083796 -0.112961  0.000671 -0.050403  ... -0.178980 -0.252651   \n",
      "2  0.038398  0.096167 -0.096763 -0.018022  0.016321  ... -0.167772 -0.270531   \n",
      "3  0.033397  0.084818 -0.119774 -0.028753 -0.030460  ... -0.165330 -0.279122   \n",
      "4  0.033620  0.057612 -0.104852 -0.013622 -0.017427  ... -0.209840 -0.229515   \n",
      "\n",
      "       1529      1530      1531      1532      1533      1534      1535  label  \n",
      "0 -0.061778  0.205731  0.027570 -0.133678 -0.054258 -0.177480  0.268748      0  \n",
      "1 -0.049785  0.185724  0.014776 -0.098063 -0.031113 -0.181574  0.291177      0  \n",
      "2 -0.109937  0.200574 -0.002043 -0.114087 -0.050619 -0.196107  0.234879      0  \n",
      "3 -0.071788  0.210830 -0.011106 -0.118139 -0.057057 -0.175628  0.264600      0  \n",
      "4 -0.065925  0.211815  0.016175 -0.121006 -0.004409 -0.176248  0.259960      0  \n",
      "\n",
      "[5 rows x 1538 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "def load_labels_from_pickle(pickle_file):\n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        # 直接访问 data 字典中的 'image_id' 和 'label'\n",
    "        image_ids = data['image_id']\n",
    "        labels = data['label']\n",
    "        labels_dict = dict(zip(image_ids, labels))\n",
    "    return labels_dict\n",
    "\n",
    "# 加载 pickle 文件\n",
    "pickle_path = 'train.pkl'\n",
    "labels_dict = load_labels_from_pickle(pickle_path)\n",
    "\n",
    "# 加载 CSV 文件\n",
    "features_path = 'fused.csv'\n",
    "features_df = pd.read_csv(features_path)\n",
    "\n",
    "# 确保 CSV 文件中最后一列为 image_id\n",
    "if features_df.columns[-1] != 'image_id':\n",
    "    print(\"Error: 'image_id' must be the last column in the CSV file.\")\n",
    "else:\n",
    "    # 创建 DataFrame 用于包含从字典中提取的 image_id 和 label\n",
    "    labels_df = pd.DataFrame(list(labels_dict.items()), columns=['image_id', 'label'])\n",
    "\n",
    "    # 将 features_df 和 labels_df 中的 'image_id' 设置为索引\n",
    "    features_df.set_index('image_id', inplace=True)\n",
    "    labels_df.set_index('image_id', inplace=True)\n",
    "\n",
    "    # 根据 image_id 合并 features_df 和 labels_df\n",
    "    merged_df = pd.merge(features_df, labels_df, left_index=True, right_index=True, how='inner')\n",
    "\n",
    "    # 重置索引以便导出或其他处理\n",
    "    merged_df.reset_index(inplace=True)\n",
    "\n",
    "    # 检查合并后的数据\n",
    "    print(merged_df.head())\n",
    "\n",
    "    # 可以选择保存合并后的 DataFrame\n",
    "    # merged_df.to_csv('D:\\\\CAPSTONE5703_CNN\\\\merged_fused features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 从DataFrame中分离出特征数据和标签\n",
    "X = merged_df.drop(['label', 'image_id'], axis=1)  # 移除 'label' 和 'image_id' 列\n",
    "y = merged_df['label']  # 标签\n",
    "\n",
    "# 打印出每列的数据类型\n",
    "# print(X.dtypes)\n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# X_train and X_test represent the training set and test set of feature data respectively.\n",
    "# y_train and y_test represent the training set and test set of labeled data respectively.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training dimensions: 193\n",
      "New testing dimensions: 193\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 应用 PCA 降维，只在训练数据上fit，然后transform训练数据和测试数据\n",
    "pca = PCA(n_components=0.95)  # 保留95%的方差\n",
    "X_train_pca = pca.fit_transform(X_train) # pca后的训练集\n",
    "X_test_pca = pca.transform(X_test) # pca后的测试集\n",
    "\n",
    "# 检查新的维数和解释的方差比\n",
    "print(\"New training dimensions:\", X_train_pca.shape[1])\n",
    "print(\"New testing dimensions:\", X_test_pca.shape[1])\n",
    "# print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best parameters:  {'solver': 'adam', 'max_iter': 800, 'learning_rate_init': 0.1, 'hidden_layer_sizes': (200, 100), 'batch_size': 64, 'alpha': 0.001, 'activation': 'relu'}\n",
      "Best score:  0.5863267192490365\n",
      "Accuracy on test set:  0.6131117266851339\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.59      0.59       517\n",
      "           1       0.63      0.63      0.63       566\n",
      "\n",
      "    accuracy                           0.61      1083\n",
      "   macro avg       0.61      0.61      0.61      1083\n",
      "weighted avg       0.61      0.61      0.61      1083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# 创建并配置MLP分类器\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200, 100), (300, 150), (100, 50)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=-1, verbose=1, n_iter=40)\n",
    "random_search.fit(X_train_pca, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best parameters:  {'solver': 'sgd', 'max_iter': 1100, 'learning_rate_init': 0.1, 'hidden_layer_sizes': (300, 150, 75), 'batch_size': 64, 'alpha': 0.001, 'activation': 'relu'}\n",
      "Best score:  0.573407278955598\n",
      "Accuracy on test set:  0.5872576177285319\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.71      0.62       517\n",
      "           1       0.64      0.47      0.55       566\n",
      "\n",
      "    accuracy                           0.59      1083\n",
      "   macro avg       0.60      0.59      0.58      1083\n",
      "weighted avg       0.60      0.59      0.58      1083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200,100,50), (300,150,75)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=-1, verbose=1, n_iter=40)\n",
    "random_search.fit(X_train_pca, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best parameters:  {'solver': 'adam', 'max_iter': 800, 'learning_rate_init': 0.001, 'hidden_layer_sizes': (200, 100, 50, 25), 'batch_size': 64, 'alpha': 0.001, 'activation': 'relu'}\n",
      "Best score:  0.5888753925697436\n",
      "Accuracy on test set:  0.6214219759926131\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.59      0.60       517\n",
      "           1       0.63      0.65      0.64       566\n",
      "\n",
      "    accuracy                           0.62      1083\n",
      "   macro avg       0.62      0.62      0.62      1083\n",
      "weighted avg       0.62      0.62      0.62      1083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200,100,50,25), (300,150,75,30)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=-1, verbose=1, n_iter=40)\n",
    "random_search.fit(X_train_pca, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.3s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.0s\n",
      "[CV] END ...................................C=10, gamma=0.01; total time=   2.1s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.5s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.6s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.5s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.5s\n",
      "[CV] END .....................................C=100, gamma=1; total time=   2.5s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   2.1s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   2.0s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   2.1s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   2.1s\n",
      "[CV] END ....................................C=1, gamma=0.01; total time=   2.0s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   2.5s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   2.5s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   2.5s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   2.4s\n",
      "[CV] END ...................................C=100, gamma=0.1; total time=   2.5s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   2.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   2.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   2.1s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   2.0s\n",
      "[CV] END ...................................C=0.1, gamma=0.1; total time=   2.0s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   2.1s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   2.1s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   2.1s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   2.1s\n",
      "[CV] END ..................................C=0.1, gamma=0.01; total time=   2.1s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   2.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   2.1s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   2.0s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   2.1s\n",
      "[CV] END .....................................C=1, gamma=0.1; total time=   2.1s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   2.0s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   2.0s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   2.0s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   2.0s\n",
      "[CV] END ....................................C=10, gamma=0.1; total time=   2.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.0s\n",
      "[CV] END .......................................C=1, gamma=1; total time=   2.4s\n",
      "[CV] END .................................C=0.1, gamma=scale; total time=   2.0s\n",
      "[CV] END .................................C=0.1, gamma=scale; total time=   2.0s\n",
      "[CV] END .................................C=0.1, gamma=scale; total time=   2.0s\n",
      "[CV] END .................................C=0.1, gamma=scale; total time=   2.3s\n",
      "[CV] END .................................C=0.1, gamma=scale; total time=   2.3s\n",
      "Best parameters found: {'gamma': 1, 'C': 100}\n",
      "Accuracy: 0.6325023084025854\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.58      0.60       517\n",
      "           1       0.64      0.68      0.66       566\n",
      "\n",
      "    accuracy                           0.63      1083\n",
      "   macro avg       0.63      0.63      0.63      1083\n",
      "weighted avg       0.63      0.63      0.63      1083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 将数据重塑为二维\n",
    "X_train_pca_reshaped = X_train_pca.reshape(X_train_pca.shape[0], X_train_pca.shape[2])\n",
    "X_test_pca_reshaped = X_test_pca.reshape(X_test_pca.shape[0], X_test_pca.shape[2])\n",
    "\n",
    "# 创建SVM模型，尝试使用RBF核\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# 设置更广泛的参数网格\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 'scale']\n",
    "}\n",
    "\n",
    "# 使用网格搜索进行调参，应用StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(svc, param_grid, refit=True, verbose=2, cv=cv)\n",
    "random_search.fit(X_train_pca_reshaped, y_train)\n",
    "\n",
    "# 预测\n",
    "predictions = random_search.predict(X_test_pca_reshaped)\n",
    "\n",
    "# 评估模型\n",
    "print(\"Best parameters found:\", random_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
      "Best parameters found: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': None, 'bootstrap': False}\n",
      "Best score: 0.6075775616590884\n",
      "Accuracy on test set: 0.6112650046168052\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.63      0.61       517\n",
      "           1       0.64      0.59      0.61       566\n",
      "\n",
      "    accuracy                           0.61      1083\n",
      "   macro avg       0.61      0.61      0.61      1083\n",
      "weighted avg       0.61      0.61      0.61      1083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 将数据重塑为二维\n",
    "X_train_pca_reshaped = X_train_pca.reshape(X_train_pca.shape[0], X_train_pca.shape[2])\n",
    "X_test_pca_reshaped = X_test_pca.reshape(X_test_pca.shape[0], X_test_pca.shape[2])\n",
    "\n",
    "# 设置参数分布\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],  # 树的数量\n",
    "    'max_depth': [None, 10, 20, 30],  # 树的最大深度\n",
    "    'min_samples_split': [2, 5, 10],  # 内部节点再划分所需最小样本数\n",
    "    'min_samples_leaf': [1, 2, 4],    # 叶子节点最小样本数\n",
    "    'bootstrap': [True, False]        # 是否有放回地抽样\n",
    "}\n",
    "\n",
    "# 初始化随机森林分类器\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 设置K折交叉验证\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 运行随机搜索\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=40, cv=cv, scoring='accuracy', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_result = random_search.fit(X_train_pca_reshaped, y_train)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Best parameters found:\", random_result.best_params_)\n",
    "print(\"Best score:\", random_result.best_score_)\n",
    "\n",
    "# 使用最佳参数训练模型并评估在测试集上的性能\n",
    "best_rf = random_result.best_estimator_\n",
    "best_rf.fit(X_train_pca_reshaped, y_train)\n",
    "y_pred = best_rf.predict(X_test_pca_reshaped)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on test set: {test_accuracy}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLP - design baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hidden -BCEWithLogitsLoss - Xavier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 246.93 seconds\n",
      "Best Training Accuracy = 99.0\n",
      "Accuracy on test set: 0.6112650046168052\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.59      0.63      0.61       517\n",
      "         1.0       0.64      0.59      0.61       566\n",
      "\n",
      "    accuracy                           0.61      1083\n",
      "   macro avg       0.61      0.61      0.61      1083\n",
      "weighted avg       0.61      0.61      0.61      1083\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# 重塑数据为二维\n",
    "X_train_pca_reshaped = X_train_pca.reshape(X_train_pca.shape[0], X_train_pca.shape[2])\n",
    "X_test_pca_reshaped = X_test_pca.reshape(X_test_pca.shape[0], X_test_pca.shape[2])\n",
    "\n",
    "# 准备数据\n",
    "X_train_tensor = torch.from_numpy(X_train_pca_reshaped).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=193):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 200)\n",
    "        self.prelu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(200, 100)\n",
    "        self.prelu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(100, 50)\n",
    "        self.prelu3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear(50, 25)\n",
    "        self.prelu4 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(25, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)  # Xavier 初始化\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu1(self.layer1(x))\n",
    "        x = self.prelu2(self.layer2(x))\n",
    "        x = self.prelu3(self.layer3(x))\n",
    "        x = self.prelu4(self.layer4(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_size=193)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)  # 使用Adam优化器\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum / y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc.item()\n",
    "\n",
    "# 开始训练\n",
    "start_time = time.time()\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(200):  # 训练200个epoch\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):  \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 计算训练集上的准确率\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train_tensor)\n",
    "        train_acc = binary_acc(train_outputs, y_train_tensor.view(-1, 1))\n",
    "        if train_acc > best_acc:\n",
    "            best_acc = train_acc\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Best Training Accuracy = {best_acc}\")\n",
    "\n",
    "# 模型评估\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "with torch.no_grad():  # 禁止跟踪计算图\n",
    "    X_test_tensor = torch.from_numpy(X_test_pca_reshaped).float() \n",
    "    y_test_tensor = torch.from_numpy(y_test).float()  # 将 NumPy 数组转换为 PyTorch 张量，并转换数据类型为浮点型\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_acc = binary_acc(test_outputs, y_test_tensor.view(-1, 1))\n",
    "    print(f\"Accuracy on test set: {test_accuracy}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test_tensor, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 hidden - without PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(1536,768,384,192,96)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [800,1100,1400],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=-1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
