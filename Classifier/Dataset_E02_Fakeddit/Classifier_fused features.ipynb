{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the fused fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0.13273102  -0.046325  0.04799986  -0.054121092  -0.26567036  0.28699276  \\\n",
      "0    0.166932  -0.040577    0.022252     -0.036686    -0.257072    0.275591   \n",
      "1    0.141127  -0.032970    0.055993     -0.034640    -0.267474    0.290403   \n",
      "2    0.172750  -0.065717   -0.025985     -0.075753    -0.179092    0.282487   \n",
      "3    0.163937  -0.015764    0.043609     -0.071291    -0.271180    0.274031   \n",
      "4    0.129085  -0.017540    0.014079     -0.054526    -0.234425    0.316496   \n",
      "\n",
      "   -0.20155007  0.15216848  -0.21968047  -0.10451735  ...  0.1426529  \\\n",
      "0    -0.190400    0.157786    -0.211556    -0.094982  ...   0.141150   \n",
      "1    -0.170492    0.175335    -0.225272    -0.110479  ...   0.135947   \n",
      "2    -0.175625    0.167229    -0.230976    -0.117548  ...   0.156261   \n",
      "3    -0.210409    0.149995    -0.210583    -0.079840  ...   0.133538   \n",
      "4    -0.179008    0.167545    -0.199602    -0.086054  ...   0.193026   \n",
      "\n",
      "   0.1861744  0.099532425  -0.15293124  0.056991853  0.14413288  0.077607326  \\\n",
      "0   0.185733     0.068675    -0.155648     0.084026    0.137559     0.070157   \n",
      "1   0.190948     0.092881    -0.185682     0.024211    0.137930     0.080483   \n",
      "2   0.154905     0.074493    -0.163727     0.052328    0.107678     0.100240   \n",
      "3   0.210502     0.088576    -0.177034     0.051924    0.143257     0.062351   \n",
      "4   0.159544     0.106622    -0.168998     0.040364    0.135270     0.125020   \n",
      "\n",
      "   0.18687658  awxhir  1  \n",
      "0    0.172644  4xypkv  1  \n",
      "1    0.191332  2cfi7g  1  \n",
      "2    0.143045  212vud  0  \n",
      "3    0.158934  15ilmc  1  \n",
      "4    0.167946  4h209r  1  \n",
      "\n",
      "[5 rows x 1538 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载特征数据\n",
    "features_path = 'fused_combined.csv'\n",
    "features_df = pd.read_csv(features_path, low_memory=False)\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "# 查看前数据\n",
    "print(features_df.head())\n",
    "# 查看label分布\n",
    "#label_distribution = features_df['label'].value_counts()\n",
    "# print(label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_columns = 1538\n",
    "columns = [f'feature_{i}' for i in range(1, num_columns - 1)] + ['image_id', 'label']\n",
    "\n",
    "# 加载特征数据并添加列名\n",
    "features_path = 'fused_combined.csv'\n",
    "features_df = pd.read_csv(features_path, header=None, names=columns, low_memory=False)\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "# 确保所有特征列都是数值类型（排除 'image_id' 和 'label' 列）\n",
    "features_df.iloc[:, :-2] = features_df.iloc[:, :-2].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 按照label进行分组\n",
    "label_0 = features_df[features_df['label'] == 0]\n",
    "label_1 = features_df[features_df['label'] == 1]\n",
    "\n",
    "# 打印每个标签的样本数量\n",
    "# print(f\"标签为0的样本数量: {label_0.shape[0]}\")\n",
    "# print(f\"标签为1的样本数量: {label_1.shape[0]}\")\n",
    "\n",
    "# 检查样本数量是否足够\n",
    "if label_0.shape[0] < 5000 or label_1.shape[0] < 5000:\n",
    "    print(\"样本数量不足，无法从每个标签中抽取5000个样本。\")\n",
    "else:\n",
    "    # 选择5000个label为0的数据和5000个label为1的数据\n",
    "    label_0_sample = label_0.sample(n=5000, random_state=42)\n",
    "    label_1_sample = label_1.sample(n=5000, random_state=42)\n",
    "\n",
    "    # 合并两个样本\n",
    "    balanced_df = pd.concat([label_0_sample, label_1_sample], ignore_index=True)\n",
    "\n",
    "    # 确保列名的顺序是正确的\n",
    "    cols = list(balanced_df.columns)\n",
    "    cols.remove('image_id')\n",
    "    cols.remove('label')\n",
    "    balanced_df = balanced_df[cols + ['image_id', 'label']]\n",
    "\n",
    "    # 打印总行数和总列数\n",
    "    print(f\"总行数: {balanced_df.shape[0]}\")\n",
    "    print(f\"总列数: {balanced_df.shape[1]}\")\n",
    "\n",
    "    # 打印前五行数据\n",
    "    # print(balanced_df.head())\n",
    "\n",
    "    # 检查是否存在任何缺失值\n",
    "    print(balanced_df.isnull().sum())\n",
    "\n",
    "    # 如果需要将数据保存到新的CSV文件中\n",
    "    # balanced_df.to_csv('balanced_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 从 DataFrame 中分离出特征数据和标签\n",
    "X = balanced_df.drop(['label', 'image_id'], axis=1)  # 移除 'label' 和 'image_id' 列\n",
    "y = balanced_df['label']  # 标签\n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 存储 image_id 以便之后合并\n",
    "image_id_train = balanced_df.loc[X_train.index, 'image_id']\n",
    "image_id_test = balanced_df.loc[X_test.index, 'image_id']\n",
    "\n",
    "# 再次确保所有数据都是数值类型\n",
    "X_train = X_train.apply(pd.to_numeric)\n",
    "X_test = X_test.apply(pd.to_numeric)\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# 统计数据的基本信息\n",
    "print(\"数据的基本统计信息:\")\n",
    "print(pd.DataFrame(X_train).describe())\n",
    "print(pd.DataFrame(X_test).describe())\n",
    "\n",
    "# 打印检查\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"Training image_ids shape:\", image_id_train.shape)\n",
    "print(\"Test image_ids shape:\", image_id_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 应用 PCA 降维，保留95%的方差\n",
    "pca1 = PCA(n_components=0.95)  \n",
    "X_train_pca1 = pca1.fit_transform(X_train) # pca后的训练集\n",
    "X_test_pca1 = pca1.transform(X_test) # pca后的测试集\n",
    "\n",
    "# 检查新的维数和解释的方差比\n",
    "print(\"New training dimensions:\", X_train_pca1.shape[1])\n",
    "print(\"New testing dimensions:\", X_test_pca1.shape[1])\n",
    "# print(\"Total explained variance:\", np.sum(pca1.explained_variance_ratio_))\n",
    "# print(\"Explained variance ratio:\", pca1.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 创建并配置MLP分类器\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(100, 50), (50, 25), (30, 15)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train_pca1, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca1, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca1)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 创建并配置MLP分类器\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200,100,50), (300,150,75)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train_pca1, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca1, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca1)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 创建并配置MLP分类器\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200,100,50,25), (300,150,75,30)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train_pca1, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca1, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca1)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 创建SVM模型，尝试使用RBF核\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# 设置更广泛的参数网格\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 'scale']\n",
    "}\n",
    "\n",
    "# 使用网格搜索进行调参，应用StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "random_search = RandomizedSearchCV(svc, param_grid, refit=True, verbose=2, cv=cv)\n",
    "random_search.fit(X_train_pca1, y_train)\n",
    "\n",
    "# 预测\n",
    "predictions = random_search.predict(X_test_pca1)\n",
    "\n",
    "# 评估模型\n",
    "print(\"Best parameters found:\",random_search.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 设置网格搜索参数\n",
    "# 设置参数分布\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],  # 树的数量\n",
    "    'max_depth': [None, 10, 20, 30],  # 树的最大深度\n",
    "    'min_samples_split': [2, 5, 10],  # 内部节点再划分所需最小样本数\n",
    "    'min_samples_leaf': [1, 2, 4],    # 叶子节点最小样本数\n",
    "    'bootstrap': [True, False]        # 是否有放回地抽样\n",
    "}\n",
    "\n",
    "# 初始化随机森林分类器\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 设置K折交叉验证\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 运行随机搜索\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=40, cv=cv, scoring='accuracy', n_jobs=-1, random_state=42, verbose=1)\n",
    "random_result = random_search.fit(X_train_pca1, y_train)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Best parameters found:\", random_result.best_params_)\n",
    "print(\"Best score:\", random_result.best_score_)\n",
    "\n",
    "# 使用最佳参数训练模型并评估在测试集上的性能\n",
    "best_rf = random_result.best_estimator_\n",
    "best_rf.fit(X_train_pca1, y_train)\n",
    "y_pred = best_rf.predict(X_test_pca1)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy on test set: {test_accuracy}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - design baseline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hidden -BCEWithLogitsLoss - Xavier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# 重塑数据为二维\n",
    "X_train_pca_reshaped = X_train_pca.reshape(X_train_pca1.shape[0], X_train_pca.shape[2])\n",
    "X_test_pca_reshaped = X_test_pca.reshape(X_test_pca1.shape[0], X_test_pca.shape[2])\n",
    "\n",
    "# 准备数据\n",
    "X_train_tensor = torch.from_numpy(X_train_pca_reshaped).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size=193):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 200)\n",
    "        self.prelu1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(200, 100)\n",
    "        self.prelu2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(100, 50)\n",
    "        self.prelu3 = nn.ReLU()\n",
    "        self.layer4 = nn.Linear(50, 25)\n",
    "        self.prelu4 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(25, 1)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for layer in self.children():\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)  # Xavier 初始化\n",
    "                if layer.bias is not None:\n",
    "                    nn.init.constant_(layer.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prelu1(self.layer1(x))\n",
    "        x = self.prelu2(self.layer2(x))\n",
    "        x = self.prelu3(self.layer3(x))\n",
    "        x = self.prelu4(self.layer4(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "model = MLP(input_size=193)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)  # 使用Adam优化器\n",
    "\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum / y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    return acc.item()\n",
    "\n",
    "# 开始训练\n",
    "start_time = time.time()\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(200):  # 训练200个epoch\n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(train_loader):  \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # 计算训练集上的准确率\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_outputs = model(X_train_tensor)\n",
    "        train_acc = binary_acc(train_outputs, y_train_tensor.view(-1, 1))\n",
    "        if train_acc > best_acc:\n",
    "            best_acc = train_acc\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Best Training Accuracy = {best_acc}\")\n",
    "\n",
    "# 模型评估\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "with torch.no_grad():  # 禁止跟踪计算图\n",
    "    X_test_tensor = torch.from_numpy(X_test_pca_reshaped).float() \n",
    "    y_test_tensor = torch.from_numpy(y_test).float()  # 将 NumPy 数组转换为 PyTorch 张量，并转换数据类型为浮点型\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_acc = binary_acc(test_outputs, y_test_tensor.view(-1, 1))\n",
    "    print(f\"Accuracy on test set: {test_accuracy}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test_tensor, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 hidden - without PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(1536,768,384,192,96)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [800,1100,1400],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=-1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
