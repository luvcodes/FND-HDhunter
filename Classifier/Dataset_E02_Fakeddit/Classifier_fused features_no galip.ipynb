{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the fused fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总行数: 10000\n",
      "总列数: 1026\n",
      "feature_0       0\n",
      "feature_1       0\n",
      "feature_2       0\n",
      "feature_3       0\n",
      "feature_4       0\n",
      "               ..\n",
      "feature_1021    0\n",
      "feature_1022    0\n",
      "feature_1023    0\n",
      "image_id        0\n",
      "label           0\n",
      "Length: 1026, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载特征数据\n",
    "features_path = 'fused_new.csv'\n",
    "features_df = pd.read_csv(features_path, low_memory=False)\n",
    "features_df = features_df.dropna()\n",
    "\n",
    "# 查看前数据\n",
    "# print(features_df.head()\n",
    "# 查看label分布\n",
    "#label_distribution = features_df['label'].value_counts()\n",
    "# print(label_distribution)\n",
    "\n",
    "# 确保所有特征列都是数值类型（排除 'image_id' 和 'label' 列）\n",
    "features_df.iloc[:, :-2] = features_df.iloc[:, :-2].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# 按照label进行分组\n",
    "label_0 = features_df[features_df['label'] == 0]\n",
    "label_1 = features_df[features_df['label'] == 1]\n",
    "\n",
    "# 打印每个标签的样本数量\n",
    "# print(f\"标签为0的样本数量: {label_0.shape[0]}\")\n",
    "# print(f\"标签为1的样本数量: {label_1.shape[0]}\")\n",
    "\n",
    "# 检查样本数量是否足够\n",
    "if label_0.shape[0] < 5000 or label_1.shape[0] < 5000:\n",
    "    print(\"样本数量不足，无法从每个标签中抽取5000个样本。\")\n",
    "else:\n",
    "    # 选择5000个label为0的数据和5000个label为1的数据\n",
    "    label_0_sample = label_0.sample(n=5000, random_state=42)\n",
    "    label_1_sample = label_1.sample(n=5000, random_state=42)\n",
    "\n",
    "    # 合并两个样本\n",
    "    balanced_df = pd.concat([label_0_sample, label_1_sample], ignore_index=True)\n",
    "\n",
    "    # 打印总行数和总列数\n",
    "    print(f\"总行数: {balanced_df.shape[0]}\")\n",
    "    print(f\"总列数: {balanced_df.shape[1]}\")\n",
    "\n",
    "    # 打印前五行数据\n",
    "    # print(balanced_df.head())\n",
    "\n",
    "    # 检查是否存在任何缺失值\n",
    "    print(balanced_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据的基本统计信息:\n",
      "               0             1             2             3             4     \\\n",
      "count  8.000000e+03  8.000000e+03  8.000000e+03  8.000000e+03  8.000000e+03   \n",
      "mean  -7.152557e-10 -2.622604e-09  1.192093e-09  3.099442e-09 -7.152557e-10   \n",
      "std    1.000062e+00  1.000062e+00  1.000062e+00  1.000062e+00  1.000062e+00   \n",
      "min   -2.451431e+00 -3.655570e+00 -3.939831e+00 -3.620988e+00 -1.264867e+00   \n",
      "25%   -3.133890e-01  2.543819e-01 -1.395582e-01  5.805079e-02 -5.512592e-01   \n",
      "50%   -1.340701e-01  3.921014e-01  2.639310e-01  3.156925e-01 -3.271695e-01   \n",
      "75%    3.212019e-02  5.027586e-01  6.014655e-01  5.108766e-01 -3.935668e-02   \n",
      "max    3.613493e+00  8.825111e-01  2.085891e+00  1.284997e+00  3.613818e+00   \n",
      "\n",
      "               5             6             7             8             9     \\\n",
      "count  8.000000e+03  8.000000e+03  8.000000e+03  8.000000e+03  8.000000e+03   \n",
      "mean  -1.430511e-09 -1.668930e-09 -1.192093e-09 -1.192093e-10  1.788139e-09   \n",
      "std    1.000062e+00  1.000062e+00  1.000062e+00  1.000062e+00  1.000062e+00   \n",
      "min   -4.081353e+00 -3.531153e+00 -1.366545e+00 -1.269460e+00 -2.960175e+00   \n",
      "25%    1.313275e-01  1.251586e-01 -5.354983e-01 -5.418101e-01  2.850065e-01   \n",
      "50%    2.754139e-01  3.514763e-01 -3.565740e-01 -3.969234e-01  4.173740e-01   \n",
      "75%    4.200834e-01  5.051599e-01 -1.060226e-01 -1.858835e-01  5.076308e-01   \n",
      "max    9.540777e-01  1.331007e+00  3.646178e+00  3.303834e+00  9.675394e-01   \n",
      "\n",
      "       ...          1014          1015          1016          1017  \\\n",
      "count  ...  8.000000e+03  8.000000e+03  8.000000e+03  8.000000e+03   \n",
      "mean   ... -1.668930e-09  9.536744e-10  1.430511e-09 -2.384186e-09   \n",
      "std    ...  1.000062e+00  1.000062e+00  1.000062e+00  1.000062e+00   \n",
      "min    ... -1.280349e+00 -1.360834e+00 -4.024068e+00 -3.604942e+00   \n",
      "25%    ... -5.547509e-01 -5.417222e-01 -7.752334e-02 -6.415299e-01   \n",
      "50%    ... -3.808931e-01 -3.493483e-01  2.609342e-01 -6.915065e-02   \n",
      "75%    ... -1.453396e-01 -6.963677e-02  5.583980e-01  6.270699e-01   \n",
      "max    ...  3.564313e+00  3.601758e+00  1.841525e+00  3.774145e+00   \n",
      "\n",
      "               1018          1019          1020          1021          1022  \\\n",
      "count  8.000000e+03  8.000000e+03  8.000000e+03  8.000000e+03  8.000000e+03   \n",
      "mean   9.536744e-10 -4.768372e-10 -9.536744e-10 -2.384186e-10 -1.907349e-09   \n",
      "std    1.000062e+00  1.000062e+00  1.000062e+00  1.000062e+00  1.000062e+00   \n",
      "min   -9.484717e-01 -4.552454e+00 -4.601968e+00 -3.710237e+00 -1.223057e+00   \n",
      "25%   -5.141375e-01 -1.818045e-01 -4.924115e-01 -1.092561e-01 -4.393935e-01   \n",
      "50%   -4.172970e-01  2.005802e-01  1.218322e-01  3.101927e-01 -2.733010e-01   \n",
      "75%   -2.612927e-01  5.754954e-01  6.698003e-01  6.246592e-01 -7.217294e-02   \n",
      "max    2.913908e+00  2.652618e+00  2.641502e+00  1.791098e+00  3.907398e+00   \n",
      "\n",
      "               1023  \n",
      "count  8.000000e+03  \n",
      "mean   1.549721e-09  \n",
      "std    1.000062e+00  \n",
      "min   -2.735050e+00  \n",
      "25%    2.094843e-01  \n",
      "50%    4.169582e-01  \n",
      "75%    5.368361e-01  \n",
      "max    1.032096e+00  \n",
      "\n",
      "[8 rows x 1024 columns]\n",
      "              0            1            2            3            4     \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean     -0.009185    -0.011099    -0.017191     0.003942    -0.023899   \n",
      "std       0.997627     1.009905     1.003477     1.010093     1.005480   \n",
      "min      -2.116159    -3.402523    -3.975114    -3.545105    -1.322068   \n",
      "25%      -0.307660     0.243880    -0.162690     0.058051    -0.572925   \n",
      "50%      -0.132969     0.385601     0.231577     0.323911    -0.358991   \n",
      "75%       0.032120     0.494526     0.596071     0.524887    -0.065617   \n",
      "max       3.674893     0.830418     1.947964     1.315623     3.641324   \n",
      "\n",
      "              5            6            7            8            9     ...  \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  ...   \n",
      "mean     -0.010909     0.007844     0.006887    -0.012352     0.006698  ...   \n",
      "std       1.015688     1.020822     0.991949     1.000302     0.988062  ...   \n",
      "min      -4.008995    -3.515295    -1.330762    -1.151271    -3.000032  ...   \n",
      "25%       0.130526     0.133817    -0.530614    -0.554007     0.287158  ...   \n",
      "50%       0.275222     0.360940    -0.344978    -0.400778     0.418408  ...   \n",
      "75%       0.412346     0.523709    -0.091802    -0.184578     0.502645  ...   \n",
      "max       0.867588     1.188366     3.602515     3.112610     0.823679  ...   \n",
      "\n",
      "              1014         1015         1016         1017         1018  \\\n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000   \n",
      "mean     -0.013502    -0.003035     0.002654    -0.013147    -0.003316   \n",
      "std       0.987965     0.987853     1.019637     1.011558     1.000030   \n",
      "min      -1.309911    -1.185973    -3.881582    -3.307590    -0.914886   \n",
      "25%      -0.554751    -0.538102    -0.067636    -0.679805    -0.514806   \n",
      "50%      -0.393215    -0.344752     0.285319    -0.076783    -0.412468   \n",
      "75%      -0.154714    -0.056969     0.562294     0.613357    -0.266795   \n",
      "max       3.068057     3.403305     1.796541     4.185560     2.895718   \n",
      "\n",
      "              1019         1020         1021         1022         1023  \n",
      "count  2000.000000  2000.000000  2000.000000  2000.000000  2000.000000  \n",
      "mean      0.007382     0.050643    -0.000129     0.002346     0.016783  \n",
      "std       0.999014     0.997319     0.980229     1.005639     1.002666  \n",
      "min      -4.808359    -3.288909    -3.443161    -1.213082    -2.696762  \n",
      "25%      -0.177462    -0.419888    -0.103191    -0.434556     0.225475  \n",
      "50%       0.202886     0.168621     0.289127    -0.281930     0.430648  \n",
      "75%       0.583100     0.725741     0.596878    -0.078124     0.551471  \n",
      "max       2.260372     2.574814     1.602002     3.642235     0.991551  \n",
      "\n",
      "[8 rows x 1024 columns]\n",
      "Training set shape: (8000, 1024)\n",
      "Test set shape: (2000, 1024)\n",
      "Training labels shape: (8000,)\n",
      "Test labels shape: (2000,)\n",
      "Training image_ids shape: (8000,)\n",
      "Test image_ids shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 从 DataFrame 中分离出特征数据和标签\n",
    "X = balanced_df.drop(['label', 'image_id'], axis=1)  # 移除 'label' 和 'image_id' 列\n",
    "y = balanced_df['label']  # 标签\n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 存储 image_id 以便之后合并\n",
    "image_id_train = balanced_df.loc[X_train.index, 'image_id']\n",
    "image_id_test = balanced_df.loc[X_test.index, 'image_id']\n",
    "\n",
    "# 再次确保所有数据都是数值类型\n",
    "X_train = X_train.apply(pd.to_numeric)\n",
    "X_test = X_test.apply(pd.to_numeric)\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# 统计数据的基本信息\n",
    "print(\"数据的基本统计信息:\")\n",
    "print(pd.DataFrame(X_train).describe())\n",
    "print(pd.DataFrame(X_test).describe())\n",
    "\n",
    "# 打印检查\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"Training image_ids shape:\", image_id_train.shape)\n",
    "print(\"Test image_ids shape:\", image_id_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training dimensions: 30\n",
      "New testing dimensions: 30\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 应用 PCA 降维，保留95%的方差\n",
    "pca1 = PCA(n_components=0.95)  \n",
    "X_train_pca1 = pca1.fit_transform(X_train) # pca后的训练集\n",
    "X_test_pca1 = pca1.transform(X_test) # pca后的测试集\n",
    "\n",
    "# 检查新的维数和解释的方差比\n",
    "print(\"New training dimensions:\", X_train_pca1.shape[1])\n",
    "print(\"New testing dimensions:\", X_test_pca1.shape[1])\n",
    "# print(\"Total explained variance:\", np.sum(pca1.explained_variance_ratio_))\n",
    "# print(\"Explained variance ratio:\", pca1.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 创建并配置MLP分类器\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(100, 50), (50, 25), (30, 15)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train_pca1, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca1, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca1)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA - 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 应用 PCA 降维\n",
    "pca2 = PCA(n_components=200)  \n",
    "X_train_pca2 = pca2.fit_transform(X_train) # pca后的训练集\n",
    "X_test_pca2 = pca2.transform(X_test) # pca后的测试集\n",
    "\n",
    "# 检查新的维数和解释的方差比\n",
    "print(\"New training dimensions:\", X_train_pca2.shape[1])\n",
    "print(\"New testing dimensions:\", X_test_pca2.shape[1])\n",
    "# print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 创建并配置MLP分类器\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200,100,50), (300,150,75)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train_pca2, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca2, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca2)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# 创建并配置MLP分类器\n",
    "mlp = MLPClassifier(random_state=42,shuffle=True,tol=1e-4,early_stopping=True, n_iter_no_change=10)\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200,100,50,25), (300,150,75,30)],  \n",
    "    'activation': ['logistic','relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001,0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'max_iter': [500,800,1100],\n",
    "    'batch_size': [64]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train_pca2, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca2, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca2)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from skorch import NeuralNetClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# 重塑数据为1D CNN接受的形状\n",
    "X_train_pca2 = X_train_pca2.reshape(X_train_pca2.shape[0], X_train_pca2.shape[1], 1)\n",
    "X_test_pca2 = X_test_pca2.reshape(X_test_pca2.shape[0], X_test_pca2.shape[1], 1)\n",
    "\n",
    "# 定义1D CNN模型\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_features, num_filters=32, kernel_size=3, pool_size=2, dense_units=128):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, num_filters, kernel_size)\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size)\n",
    "        self.fc1 = nn.Linear(num_filters*2 * ((num_features - 2 * (kernel_size - 1)) // pool_size**2), dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2])\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# 创建skorch的NeuralNetClassifier\n",
    "net = NeuralNetClassifier(\n",
    "    module=CNN1D,\n",
    "    module__num_features=X_train_pca2.shape[1],\n",
    "    max_epochs=10,\n",
    "    lr=0.01,\n",
    "    optimizer=optim.Adam,\n",
    "    optimizer__weight_decay=0.0001,\n",
    "    criterion=nn.CrossEntropyLoss,\n",
    "    batch_size=64,\n",
    "    iterator_train__shuffle=True,\n",
    "    callbacks=[skorch.callbacks.EarlyStopping(patience=5)],\n",
    "    # device='cuda' \n",
    ")\n",
    "\n",
    "# 设置网格搜索参数\n",
    "param_grid = {\n",
    "    'optimizer': [optim.Adam, optim.SGD, optim.RMSprop],\n",
    "    'module__num_filters': [32, 64, 128],\n",
    "    'module__kernel_size': [3, 5, 7],\n",
    "    'module__pool_size': [2, 3],\n",
    "    'module__dense_units': [64, 128, 256]\n",
    "}\n",
    "\n",
    "# 设置K折交叉验证\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 运行网格搜索\n",
    "grid = GridSearchCV(estimator=net, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "grid_result = grid.fit(X_train_pca2, y_train)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Best parameters found:\", grid_result.best_params_)\n",
    "print(\"Best accuracy found:\", grid_result.best_score_)\n",
    "\n",
    "# 使用最佳模型预测并评估\n",
    "best_model = grid_result.best_estimator_\n",
    "y_pred = best_model.predict(X_test_pca2)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
