{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the fused fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总行数: 10000\n",
      "总列数: 1538\n",
      "   feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0   0.135596  -0.059194  -0.061392  -0.063409  -0.198597   0.316938   \n",
      "1   0.150803  -0.027447   0.003386  -0.048853  -0.257534   0.257739   \n",
      "2   0.175314  -0.031636  -0.016272  -0.027421  -0.212084   0.295495   \n",
      "3   0.111710  -0.053619   0.015866  -0.040812  -0.238548   0.307927   \n",
      "4   0.126477  -0.029501   0.012588  -0.072348  -0.213603   0.290125   \n",
      "\n",
      "   feature_6  feature_7  feature_8  feature_9  ...  feature_1528  \\\n",
      "0  -0.179819   0.119753  -0.207725  -0.100630  ...      0.178181   \n",
      "1  -0.183425   0.165298  -0.256108  -0.106024  ...      0.135101   \n",
      "2  -0.196441   0.161346  -0.269195  -0.084032  ...      0.197769   \n",
      "3  -0.155139   0.151456  -0.222932  -0.083256  ...      0.195484   \n",
      "4  -0.204443   0.161073  -0.224686  -0.122573  ...      0.188609   \n",
      "\n",
      "   feature_1529  feature_1530  feature_1531  feature_1532  feature_1533  \\\n",
      "0      0.146170      0.048220     -0.137705      0.023962      0.108433   \n",
      "1      0.228680      0.086842     -0.198230      0.033279      0.104201   \n",
      "2      0.157627      0.065994     -0.199384      0.071631      0.107811   \n",
      "3      0.192056      0.111395     -0.161168      0.009304      0.121822   \n",
      "4      0.132541      0.041725     -0.146465      0.073054      0.109251   \n",
      "\n",
      "   feature_1534  feature_1535  image_id  label  \n",
      "0      0.118215      0.124365    1lr3di      0  \n",
      "1      0.067543      0.179575    605pg4      0  \n",
      "2      0.084018      0.192097    5gjxi7      0  \n",
      "3      0.104364      0.181840    4hzke6      0  \n",
      "4      0.100011      0.155854    7fbfv2      0  \n",
      "\n",
      "[5 rows x 1538 columns]\n",
      "feature_0       0\n",
      "feature_1       0\n",
      "feature_2       0\n",
      "feature_3       0\n",
      "feature_4       0\n",
      "               ..\n",
      "feature_1533    0\n",
      "feature_1534    0\n",
      "feature_1535    0\n",
      "image_id        0\n",
      "label           0\n",
      "Length: 1538, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载特征数据\n",
    "features_path = 'fused_combined.csv'\n",
    "features_df = pd.read_csv(features_path, header=None, low_memory=False)\n",
    "\n",
    "# 获取总列数\n",
    "num_cols = features_df.shape[1]\n",
    "\n",
    "# 设置列名,倒数第二列是image_id,最后一列是label\n",
    "col_names = [f'feature_{i}' for i in range(num_cols - 2)] + ['image_id', 'label']\n",
    "features_df.columns = col_names\n",
    "\n",
    "# 确保所有特征列都是数值类型\n",
    "for col in col_names[:-2]:  # 排除 'image_id' 和 'label'\n",
    "    features_df[col] = pd.to_numeric(features_df[col], errors='coerce')\n",
    "\n",
    "# 按照label进行分组\n",
    "label_0 = features_df[features_df['label'] == 0]\n",
    "label_1 = features_df[features_df['label'] == 1]\n",
    "\n",
    "# 选择5000个label为0的数据和5000个label为1的数据\n",
    "label_0_sample = label_0.sample(n=5000, random_state=42)\n",
    "label_1_sample = label_1.sample(n=5000, random_state=42)\n",
    "\n",
    "# 合并两个样本\n",
    "balanced_df = pd.concat([label_0_sample, label_1_sample], ignore_index=True)\n",
    "\n",
    "# 打印总行数和总列数\n",
    "print(f\"总行数: {balanced_df.shape[0]}\")\n",
    "print(f\"总列数: {balanced_df.shape[1]}\")\n",
    "\n",
    "# 打印前五行数据\n",
    "print(balanced_df.head())\n",
    "\n",
    "# 检查是否存在任何缺失值\n",
    "print(balanced_df.isnull().sum())\n",
    "\n",
    "# 如果存在缺失值,可以选择填充或删除缺失值\n",
    "# 这里选择填充缺失值为0\n",
    "balanced_df = balanced_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (8000, 1536)\n",
      "Test set shape: (2000, 1536)\n",
      "Training labels shape: (8000,)\n",
      "Test labels shape: (2000,)\n",
      "Training image_ids shape: (8000,)\n",
      "Test image_ids shape: (2000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# 从 DataFrame 中分离出特征数据和标签\n",
    "X = balanced_df.drop(['label', 'image_id'], axis=1)  # 移除 'label' 和 'image_id' 列\n",
    "y = balanced_df['label']  # 标签\n",
    "\n",
    "# 将数据集划分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 存储 image_id 以便之后合并\n",
    "image_id_train = balanced_df.loc[X_train.index, 'image_id']\n",
    "image_id_test = balanced_df.loc[X_test.index, 'image_id']\n",
    "\n",
    "# 再次确保所有数据都是数值类型\n",
    "X_train = X_train.apply(pd.to_numeric)\n",
    "X_test = X_test.apply(pd.to_numeric)\n",
    "\n",
    "# 标准化特征数据\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# 打印检查\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"Training labels shape:\", y_train.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)\n",
    "print(\"Training image_ids shape:\", image_id_train.shape)\n",
    "print(\"Test image_ids shape:\", image_id_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New training dimensions: 187\n",
      "New testing dimensions: 187\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# 应用 PCA 降维，保留95%的方差\n",
    "pca = PCA(n_components=0.95)  #\n",
    "X_train_pca = pca.fit_transform(X_train) # pca后的训练集\n",
    "X_test_pca = pca.transform(X_test) # pca后的测试集\n",
    "\n",
    "# 检查新的维数和解释的方差比\n",
    "print(\"New training dimensions:\", X_train_pca.shape[1])\n",
    "print(\"New testing dimensions:\", X_test_pca.shape[1])\n",
    "# print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP - scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- 5 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.callbacks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_92/2954343495.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural_network\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 创建Early Stopping回调函数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.callbacks'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# 创建Early Stopping回调函数\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "# 创建并配置MLP分类器,添加Early Stopping回调函数\n",
    "mlp = MLPClassifier(random_state=42, callbacks=[early_stopping])\n",
    "\n",
    "# 定义需要优化的超参数和对应的值\n",
    "param_distributions = {\n",
    "    'hidden_layer_sizes': [(200,),(200,100,),(200,100,50),(200,100,50,25),(200, 100, 50, 25, 12)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.001, 0.01],\n",
    "    'learning_rate_init': [0.0001,0.001, 0.01,0.1,1],\n",
    "    'max_iter': [500, 800]\n",
    "}\n",
    "\n",
    "# 创建RandomizedSearchCV对象\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_distributions, cv=5, scoring='accuracy', n_jobs=1, verbose=1, n_iter=10)\n",
    "random_search.fit(X_train_pca, y_train)  # 使用降维后的训练数据\n",
    "\n",
    "# 打印最佳超参数和对应的分数\n",
    "print(\"Best parameters: \", random_search.best_params_)\n",
    "print(\"Best score: \", random_search.best_score_)\n",
    "\n",
    "# 使用最佳超参数创建新的MLP模型\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# 在完整的训练集上训练最佳模型\n",
    "best_mlp.fit(X_train_pca, y_train)\n",
    "\n",
    "# 在测试集上评估最佳模型\n",
    "predictions = best_mlp.predict(X_test_pca)\n",
    "print(\"Accuracy on test set: \", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 加载数据\n",
    "data = pd.read_csv('merge_df.csv')\n",
    "X = data.drop('label', axis=1).values\n",
    "y = data['label'].values\n",
    "\n",
    "# 重塑数据为1D CNN接受的形状\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# 定义1D CNN模型\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, num_features, num_filters=32, kernel_size=3, pool_size=2, dense_units=128):\n",
    "        super(CNN1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, num_filters, kernel_size)\n",
    "        self.pool = nn.MaxPool1d(pool_size)\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters*2, kernel_size)\n",
    "        self.conv3 = nn.Conv1d(num_filters*2, num_filters*4, kernel_size)\n",
    "        self.conv4 = nn.Conv1d(num_filters*4, num_filters*8, kernel_size)\n",
    "        self.fc1 = nn.Linear(num_filters*8 * ((num_features - 4 * (kernel_size - 1)) // pool_size**4), dense_units)\n",
    "        self.fc2 = nn.Linear(dense_units, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = self.pool(self.relu(self.conv3(x)))\n",
    "        x = self.pool(self.relu(self.conv4(x)))\n",
    "        x = x.view(-1, x.shape[1] * x.shape[2])\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# 创建skorch的NeuralNetClassifier\n",
    "net = NeuralNetClassifier(\n",
    "    module=CNN1D,\n",
    "    module__num_features=X.shape[1],\n",
    "    max_epochs=10,\n",
    "    lr=0.01,\n",
    "    optimizer=optim.Adam,\n",
    "    iterator_train__shuffle=True,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# 设置网格搜索参数\n",
    "param_grid = {\n",
    "    'optimizer': [optim.Adam, optim.SGD, optim.RMSprop],\n",
    "    'module__num_filters': [32, 64, 128],\n",
    "    'module__kernel_size': [3, 5, 7],\n",
    "    'module__pool_size': [2, 3],\n",
    "    'module__dense_units': [64, 128, 256]\n",
    "}\n",
    "\n",
    "# 设置K折交叉验证\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 运行网格搜索\n",
    "grid = GridSearchCV(estimator=net, param_grid=param_grid, cv=cv, scoring='accuracy')\n",
    "grid_result = grid.fit(X, y)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Best parameters found:\", grid_result.best_params_)\n",
    "print(\"Best accuracy found:\", grid_result.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 创建SVM模型，尝试使用RBF核\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# 设置更广泛的参数网格\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.01, 0.1, 1, 'scale']\n",
    "}\n",
    "\n",
    "# 使用网格搜索进行调参，应用StratifiedKFold\n",
    "cv = StratifiedKFold(n_splits=5)\n",
    "grid = GridSearchCV(svc, param_grid, refit=True, verbose=2, cv=cv)\n",
    "grid.fit(X_train_pca, y_train)\n",
    "\n",
    "# 预测\n",
    "predictions = grid.predict(X_test_pca)\n",
    "\n",
    "# 评估模型\n",
    "print(\"Best parameters found:\", grid.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, predictions))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载数据\n",
    "# 假设 X_train_pca 和 y_train_pca 已经预处理并准备好了\n",
    "\n",
    "# 设置网格搜索参数\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],  # 树的数量\n",
    "    'max_depth': [None, 10, 20, 30],  # 树的最大深度\n",
    "    'min_samples_split': [2, 5, 10],  # 内部节点再划分所需最小样本数\n",
    "    'min_samples_leaf': [1, 2, 4],    # 叶子节点最小样本数\n",
    "    'bootstrap': [True, False]        # 是否有放回地抽样\n",
    "}\n",
    "\n",
    "# 初始化随机森林分类器\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# 设置K折交叉验证\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# 运行网格搜索\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_result = grid_search.fit(X_train_pca, y_train_pca)\n",
    "\n",
    "# 输出结果\n",
    "print(\"Best parameters found:\", grid_result.best_params_)\n",
    "print(\"Best accuracy found:\", grid_result.best_score_)\n",
    "\n",
    "# 使用最佳参数训练模型并评估在测试集上的性能\n",
    "best_rf = grid_result.best_estimator_\n",
    "best_rf.fit(X_train_pca, y_train_pca)\n",
    "y_pred = best_rf.predict(X_test_pca)\n",
    "test_accuracy = accuracy_score(y_test_pca, y_pred)\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
